# Data Collection Project Summary

## Overview

This project collects real-time data about what people around the world are:
- **Laughing about** (Memes)
- **Worried about** (Economics, Politics, News)
- **Searching for** (Google Trends)
- **Watching** (YouTube videos & Shorts, Sports)

The data powers a 3D globe visualization showing the mood and interests of different countries.

## Project Structure

```
data-collection/
â”œâ”€â”€ README.md                      # Main documentation
â”œâ”€â”€ PROJECT_SUMMARY.md             # This file
â”œâ”€â”€ FRONTEND_INTEGRATION.md        # Frontend integration guide
â”œâ”€â”€ common_schema.json             # Unified data schema
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ run_all_scrapers.py           # Master orchestration script
â”œâ”€â”€ upload_to_supabase.py         # Database upload script
â”‚
â”œâ”€â”€ memes/                         # What people are laughing about
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ scraper.py            # Reddit, Twitter, Instagram, TikTok memes
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ output_schema.json
â”‚   â””â”€â”€ output/
â”‚       â””â”€â”€ {country}/
â”‚           â””â”€â”€ {date}.json
â”‚
â”œâ”€â”€ economics/                     # What people are worried about (money)
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ scraper.py            # Financial news, economic indicators
â”‚   â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ output/
â”‚
â”œâ”€â”€ politics/                      # What people are worried about (politics)
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ scraper.py            # Political news, discussions
â”‚   â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ output/
â”‚
â”œâ”€â”€ news/                          # General current events
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ scraper.py            # News aggregators, major outlets
â”‚   â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ output/
â”‚
â”œâ”€â”€ google-trends/                 # What people are searching for
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ scraper.py            # Google Trends API, trending searches
â”‚   â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ output/
â”‚
â”œâ”€â”€ youtube/                       # What people are watching
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ scraper.py            # Trending videos, YouTube Shorts
â”‚   â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ output/
â”‚
â””â”€â”€ sports/                        # Sports news and events
    â”œâ”€â”€ scripts/
    â”‚   â””â”€â”€ scraper.py            # Sports news, scores, highlights
    â”œâ”€â”€ schemas/
    â””â”€â”€ output/
```

## Data Categories

| Category | What It Shows | Data Sources | Refresh Rate |
|----------|---------------|--------------|--------------|
| **Memes** | What people are laughing about | Reddit, Twitter, Instagram, TikTok | 2 hours |
| **Economics** | Economic worries & concerns | Financial news, World Bank, Reddit, Twitter | 6 hours |
| **Politics** | Political discussions & sentiment | News outlets, Reddit, Twitter | 2 hours |
| **News** | General current events | NewsAPI, Google News, major outlets | 1 hour |
| **Google Trends** | What people are searching | Google Trends API, YouTube trending | 30 min |
| **YouTube** | What people are watching | YouTube API, trending videos/Shorts | 1 hour |
| **Sports** | Sports news & events | ESPN, sports outlets, Reddit, Twitter | 2 hours |

## Data Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA COLLECTION                         â”‚
â”‚                                                              â”‚
â”‚  Reddit  Twitter  YouTube  Google  News APIs  Instagram     â”‚
â”‚    â”‚        â”‚       â”‚        â”‚         â”‚          â”‚         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                           â”‚                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SCRAPER SCRIPTS (Python)                    â”‚
â”‚                                                              â”‚
â”‚  â€¢ Fetch data from APIs                                     â”‚
â”‚  â€¢ Normalize & clean data                                   â”‚
â”‚  â€¢ Sentiment analysis                                       â”‚
â”‚  â€¢ Calculate virality scores                                â”‚
â”‚  â€¢ Extract trending topics                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   JSON OUTPUT FILES                          â”‚
â”‚                                                              â”‚
â”‚  {category}/output/{country}/{date}.json                    â”‚
â”‚                                                              â”‚
â”‚  Example: memes/output/USA/2025-10-25.json                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               SUPABASE (PostgreSQL Database)                 â”‚
â”‚                                                              â”‚
â”‚  Tables:                                                     â”‚
â”‚  â€¢ countries                                                 â”‚
â”‚  â€¢ category_data                                            â”‚
â”‚  â€¢ content_items                                            â”‚
â”‚  â€¢ mood_metrics                                             â”‚
â”‚  â€¢ trending_topics                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  NEXT.JS FRONTEND (earth-3d)                 â”‚
â”‚                                                              â”‚
â”‚  â€¢ Fetch data from Supabase                                 â”‚
â”‚  â€¢ Display on 3D globe                                      â”‚
â”‚  â€¢ CountryInfoPanel shows detailed data                     â”‚
â”‚  â€¢ Real-time updates (optional)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Common Data Schema

Every scraper outputs data in this format:

```json
{
  "country": "USA",
  "category": "memes|economics|politics|news|google_trends|youtube|sports",
  "timestamp": "2025-10-25T20:00:00Z",
  "last_updated": "2025-10-25T20:00:00Z",
  "items": [
    {
      "id": "unique_id",
      "title": "Item title",
      "content": "Item content/description",
      "source_platform": "reddit|twitter|youtube|etc",
      "source_url": "https://...",
      "created_at": "2025-10-25T18:00:00Z",
      "engagement": {
        "likes": 15000,
        "comments": 500,
        "shares": 2000,
        "views": 100000
      },
      "sentiment": "positive|negative|neutral",
      "virality_score": 85
    }
  ],
  "summary": {
    "total_items": 50,
    "avg_engagement": 12500,
    "overall_sentiment": "positive",
    "top_topics": ["topic1", "topic2", "topic3"]
  }
}
```

## Implementation Phases

### âœ… Phase 1: Planning & Structure (COMPLETED)
- [x] Create folder structure
- [x] Document scraping requirements for each category
- [x] Design unified data schema
- [x] Plan frontend integration
- [x] Create master orchestration script
- [x] Create database upload script

### ğŸ”„ Phase 2: Core Infrastructure (NEXT)
- [ ] Set up API credentials (Reddit, Twitter, YouTube, etc.)
- [ ] Create base scraper class with common functionality
- [ ] Set up Supabase project and database schema
- [ ] Implement data validation and error handling
- [ ] Set up logging and monitoring

### ğŸ“‹ Phase 3: Implement Scrapers
- [ ] Memes scraper (leverage existing YouTube Shorts pipeline)
- [ ] Economics scraper
- [ ] Politics scraper
- [ ] News scraper
- [ ] Google Trends scraper
- [ ] YouTube scraper
- [ ] Sports scraper

### ğŸ“‹ Phase 4: Data Processing
- [ ] Sentiment analysis pipeline
- [ ] Virality score calculation
- [ ] Topic extraction and trending analysis
- [ ] Data aggregation and summarization
- [ ] Mood meter calculation

### ğŸ“‹ Phase 5: Database & Frontend
- [ ] Create Supabase tables
- [ ] Test data upload to Supabase
- [ ] Update frontend TypeScript interfaces
- [ ] Create API functions to fetch data
- [ ] Update CountryInfoPanel component
- [ ] Add loading states and error handling

### ğŸ“‹ Phase 6: Automation & Production
- [ ] Set up cron jobs for scheduled scraping
- [ ] Implement rate limiting
- [ ] Add monitoring and alerts
- [ ] Set up caching layer
- [ ] Deploy scrapers to cloud (AWS Lambda, GCP, etc.)

## Getting Started

### 1. Install Dependencies

```bash
cd data-collection
pip install -r requirements.txt
```

### 2. Set Up Environment Variables

Create a `.env` file:

```bash
# API Keys
REDDIT_CLIENT_ID=your_reddit_client_id
REDDIT_CLIENT_SECRET=your_reddit_client_secret
REDDIT_USER_AGENT=your_user_agent

TWITTER_BEARER_TOKEN=your_twitter_bearer_token

YOUTUBE_API_KEY=your_youtube_api_key

NEWSAPI_KEY=your_newsapi_key

# Supabase
SUPABASE_URL=your_supabase_url
SUPABASE_SERVICE_KEY=your_supabase_service_key

# OpenAI (for sentiment analysis)
OPENAI_API_KEY=your_openai_api_key
```

### 3. Implement First Scraper

Start with memes since you already have the YouTube Shorts pipeline:

```bash
cd memes/scripts
# Edit scraper.py and implement the scraping logic
python scraper.py --country USA
```

### 4. Test Data Upload

```bash
python upload_to_supabase.py --category memes --country USA
```

### 5. Run All Scrapers

```bash
python run_all_scrapers.py --mode quick
```

## Countries to Track

**High Priority:**
- USA, UK, Canada, India, Australia

**Medium Priority:**
- Germany, France, Japan, Brazil

**Low Priority:**
- Mexico, Spain, Italy, Netherlands, South Korea

## API Keys Required

| Service | Purpose | Free Tier | Cost |
|---------|---------|-----------|------|
| Reddit API | Memes, discussions | âœ… 60 req/min | Free |
| Twitter API | Tweets, trends | âš ï¸ Limited | $100/mo |
| YouTube API | Videos, Shorts | âœ… 10k units/day | Free |
| NewsAPI | News articles | âœ… 100 req/day | $449/mo |
| Google Trends | Trending searches | âœ… Unlimited | Free |
| Supabase | Database | âœ… 500MB | Free |
| OpenAI | Sentiment analysis | âš ï¸ Limited | Pay-as-go |

## Success Metrics

- **Data Coverage**: 15+ countries across 7 categories
- **Update Frequency**: Data refreshed every 30 min - 6 hours
- **Data Quality**: 90%+ successful scrapes
- **Frontend Performance**: <2s load time
- **User Engagement**: Interactive globe with real-time data

## Next Steps

1. **Set up Supabase project** - Create database and tables
2. **Get API keys** - Sign up for all required services
3. **Implement memes scraper** - Start with one category
4. **Test end-to-end flow** - Scrape â†’ Store â†’ Display
5. **Scale to all categories** - Implement remaining scrapers
6. **Automate & Deploy** - Set up scheduled jobs

## Notes

- Start simple: One category, one country
- Iterate quickly: Get something working end-to-end
- Scale gradually: Add more categories and countries
- Monitor usage: Watch API quotas and costs
- Handle errors: Implement retry logic and fallbacks
- Cache aggressively: Reduce API calls and costs

## Resources

- [Reddit API Docs](https://www.reddit.com/dev/api/)
- [Twitter API Docs](https://developer.twitter.com/en/docs)
- [YouTube Data API](https://developers.google.com/youtube/v3)
- [NewsAPI Docs](https://newsapi.org/docs)
- [Google Trends API (pytrends)](https://github.com/GeneralMills/pytrends)
- [Supabase Docs](https://supabase.com/docs)

---

**Created:** October 25, 2025
**Status:** Phase 1 Complete - Ready for Implementation
