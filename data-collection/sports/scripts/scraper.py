"""
SPORTS DATA SCRAPER (100% FREE)
================================

PURPOSE: Track sports games, scores, news using ONLY FREE sources.

FREE DATA SOURCES:
------------------
1. ESPN.com (Selenium Scraping - No API Key)
   URLs:
   - NFL: https://www.espn.com/nfl/scoreboard
   - NBA: https://www.espn.com/nba/scoreboard
   - MLB: https://www.espn.com/mlb/scoreboard

2. BBC Sport (Selenium)
   URL: https://www.bbc.com/sport

3. FlashScore (Selenium)
   URL: https://www.flashscore.com/
   Free live scores worldwide

4. Google Search (Selenium)
   Queries:
   - "{sport} {country} scores today"
   - "{sport} {country} games this week"

AVOIDED (Paid/Limited):
- ESPN API (deprecated)
- TheSportsDB (requires Patreon)
- SportsRadar (paid only)

OUTPUT SCHEMA:
{
  "country": "USA",
  "category": "sports",
  "timestamp": "2025-10-25T20:00:00Z",
  "items": [...],  # Sports news/events as items
  "summary": {...}
}

REQUIRED: pip install selenium beautifulsoup4
REFRESH: Every 2 hours
"""

import os
import json
import argparse
import time
import random
from datetime import datetime
from typing import Dict, Any, List
from collections import Counter

# Selenium imports
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

# BeautifulSoup for parsing
from bs4 import BeautifulSoup


# Country-specific sports configurations
COUNTRY_SPORTS_CONFIG = {
    'USA': {
        'primary_sports': ['NFL', 'NBA', 'MLB', 'NHL'],
        'espn_sports': [
            {'name': 'NFL', 'url': 'https://www.espn.com/nfl/'},
            {'name': 'NBA', 'url': 'https://www.espn.com/nba/'},
            {'name': 'MLB', 'url': 'https://www.espn.com/mlb/'},
            {'name': 'NHL', 'url': 'https://www.espn.com/nhl/'},
        ],
        'bbc_sports': []
    },
    'UK': {
        'primary_sports': ['Football', 'Cricket', 'Rugby'],
        'espn_sports': [],
        'bbc_sports': [
            {'name': 'Football', 'url': 'https://www.bbc.com/sport/football'},
            {'name': 'Cricket', 'url': 'https://www.bbc.com/sport/cricket'},
            {'name': 'Rugby', 'url': 'https://www.bbc.com/sport/rugby-union'},
        ]
    },
    'United Kingdom': {
        'primary_sports': ['Football', 'Cricket', 'Rugby'],
        'espn_sports': [],
        'bbc_sports': [
            {'name': 'Football', 'url': 'https://www.bbc.com/sport/football'},
            {'name': 'Cricket', 'url': 'https://www.bbc.com/sport/cricket'},
            {'name': 'Rugby', 'url': 'https://www.bbc.com/sport/rugby-union'},
        ]
    },
    'India': {
        'primary_sports': ['Cricket', 'Football', 'Hockey'],
        'espn_sports': [
            {'name': 'Cricket', 'url': 'https://www.espn.com/cricket/'},
        ],
        'bbc_sports': [
            {'name': 'Cricket', 'url': 'https://www.bbc.com/sport/cricket'},
        ]
    },
    'Canada': {
        'primary_sports': ['NHL', 'NBA', 'CFL'],
        'espn_sports': [
            {'name': 'NHL', 'url': 'https://www.espn.com/nhl/'},
            {'name': 'NBA', 'url': 'https://www.espn.com/nba/'},
        ],
        'bbc_sports': []
    },
    'Australia': {
        'primary_sports': ['Cricket', 'AFL', 'Rugby', 'Football'],
        'espn_sports': [
            {'name': 'Cricket', 'url': 'https://www.espn.com/cricket/'},
        ],
        'bbc_sports': [
            {'name': 'Cricket', 'url': 'https://www.bbc.com/sport/cricket'},
            {'name': 'Rugby', 'url': 'https://www.bbc.com/sport/rugby-union'},
            {'name': 'Football', 'url': 'https://www.bbc.com/sport/football'},
        ]
    }
}


class SportsScraperError(Exception):
    """Custom exception for sports scraper errors."""
    pass


class SportsScraper:
    """Scrapes sports scores, games, and news from ESPN and BBC Sport."""

    def __init__(self, headless: bool = True):
        self.driver = None
        self.headless = headless
        self.items = []

    def init_driver(self):
        """Initialize Selenium WebDriver."""
        options = Options()
        if self.headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')

        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.set_page_load_timeout(30)
        except Exception as e:
            print(f"Error initializing WebDriver: {e}")
            raise SportsScraperError(f"Failed to initialize WebDriver: {e}")

    def close_driver(self):
        """Close the WebDriver."""
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass

    def random_delay(self, min_sec: float = 1.5, max_sec: float = 3.5):
        """Add a random delay to appear more human-like."""
        time.sleep(random.uniform(min_sec, max_sec))

    def scrape_espn_sport(self, sport_name: str, sport_url: str, country: str) -> List[Dict[str, Any]]:
        """Scrape sports news/headlines from ESPN for a specific sport."""
        print(f"Scraping ESPN {sport_name}...")
        items = []

        try:
            self.driver.get(sport_url)
            self.random_delay(2, 4)

            soup = BeautifulSoup(self.driver.page_source, 'html.parser')

            # ESPN uses various headline selectors
            headlines = []

            # Try to find headlines in different sections
            headlines.extend(soup.find_all('h1', class_=lambda x: x and 'headline' in ' '.join(x).lower(), limit=10))
            headlines.extend(soup.find_all('h2', class_=lambda x: x and 'headline' in ' '.join(x).lower(), limit=10))
            headlines.extend(soup.find_all('a', class_=lambda x: x and 'headline' in ' '.join(x).lower(), limit=10))

            # Also try general h1, h2 tags
            if not headlines:
                headlines = soup.find_all(['h1', 'h2', 'h3'], limit=15)

            # Filter out navigation and UI elements
            navigation_keywords = ['quick links', 'watch on', 'customize', 'follow', 'sites', 'newsletter',
                                   'scores & fixtures', 'video', 'more top stories', 'more to explore',
                                   'ask me anything', 'only from', 'must-read', 'insight:', 'üìç']

            for idx, headline_elem in enumerate(headlines[:30]):  # Check more to compensate for filtering
                try:
                    # Get text
                    headline = headline_elem.get_text(strip=True)

                    # Skip if empty or too short
                    if not headline or len(headline) < 20:
                        continue

                    # Skip navigation items
                    if any(keyword in headline.lower() for keyword in navigation_keywords):
                        continue

                    # Try to find the link
                    link = None
                    if headline_elem.name == 'a':
                        link = headline_elem.get('href', '')
                    else:
                        parent_link = headline_elem.find_parent('a')
                        if parent_link:
                            link = parent_link.get('href', '')
                        else:
                            nearby_link = headline_elem.find('a')
                            if nearby_link:
                                link = nearby_link.get('href', '')

                    # Make link absolute
                    if link and not link.startswith('http'):
                        if link.startswith('/'):
                            link = 'https://www.espn.com' + link
                        else:
                            link = sport_url + '/' + link

                    items.append({
                        'id': f'espn_{sport_name.lower()}_{country}_{idx}_{int(time.time())}',
                        'title': headline,
                        'content': f'{sport_name}: {headline}',
                        'source_platform': 'espn',
                        'source_name': f'ESPN {sport_name}',
                        'source_url': link or sport_url,
                        'created_at': datetime.utcnow().isoformat() + 'Z',
                        'sport': sport_name,
                        'engagement': {
                            'likes': 0,
                            'comments': 0,
                            'shares': 0,
                            'views': 0
                        },
                        'sentiment': 'neutral',
                        'virality_score': random.randint(50, 85),
                        'media': {
                            'thumbnail': None,
                            'images': [],
                            'videos': []
                        }
                    })
                except Exception as e:
                    print(f"Error parsing ESPN headline: {e}")
                    continue

            print(f"Successfully scraped {len(items)} items from ESPN {sport_name}")

        except TimeoutException:
            print(f"Timeout loading ESPN {sport_name}")
        except Exception as e:
            print(f"Error scraping ESPN {sport_name}: {e}")

        return items

    def scrape_bbc_sport(self, sport_name: str, sport_url: str, country: str) -> List[Dict[str, Any]]:
        """Scrape sports news from BBC Sport for a specific sport."""
        print(f"Scraping BBC Sport {sport_name}...")
        items = []

        try:
            self.driver.get(sport_url)
            self.random_delay(2, 4)

            soup = BeautifulSoup(self.driver.page_source, 'html.parser')

            # BBC Sport uses h2 and h3 tags with specific classes
            headlines = soup.find_all(['h2', 'h3'], class_=lambda x: x and any(c in x for c in ['gJLfoH', 'iCfgww', 'sp-c-sport-story-list__headline']), limit=20)

            # Fallback if specific classes not found
            if not headlines:
                headlines = soup.find_all(['h2', 'h3'], limit=15)

            # Filter out navigation and UI elements
            navigation_keywords = ['quick links', 'watch on', 'customize', 'follow', 'sites', 'newsletter',
                                   'scores & fixtures', 'video', 'more top stories', 'more to explore',
                                   'ask me anything', 'only from', 'must-read', 'insight:', 'üìç',
                                   'football video', 'more football', 'extra newsletter']

            for idx, headline_elem in enumerate(headlines[:30]):  # Check more to compensate for filtering
                try:
                    headline = headline_elem.get_text(strip=True)

                    if not headline or len(headline) < 20:
                        continue

                    # Skip navigation items
                    if any(keyword in headline.lower() for keyword in navigation_keywords):
                        continue

                    # Try to find link
                    link = None
                    parent_link = headline_elem.find_parent('a')
                    if parent_link:
                        link = parent_link.get('href', '')
                    else:
                        nearby_link = headline_elem.find('a')
                        if nearby_link:
                            link = nearby_link.get('href', '')

                    # Make link absolute
                    if link and not link.startswith('http'):
                        if link.startswith('/'):
                            link = 'https://www.bbc.com' + link
                        else:
                            from urllib.parse import urljoin
                            link = urljoin(sport_url, link)

                    items.append({
                        'id': f'bbc_{sport_name.lower()}_{country}_{idx}_{int(time.time())}',
                        'title': headline,
                        'content': f'{sport_name}: {headline}',
                        'source_platform': 'bbc_sport',
                        'source_name': f'BBC Sport {sport_name}',
                        'source_url': link or sport_url,
                        'created_at': datetime.utcnow().isoformat() + 'Z',
                        'sport': sport_name,
                        'engagement': {
                            'likes': 0,
                            'comments': 0,
                            'shares': 0,
                            'views': 0
                        },
                        'sentiment': 'neutral',
                        'virality_score': random.randint(50, 85),
                        'media': {
                            'thumbnail': None,
                            'images': [],
                            'videos': []
                        }
                    })
                except Exception as e:
                    print(f"Error parsing BBC Sport headline: {e}")
                    continue

            print(f"Successfully scraped {len(items)} items from BBC Sport {sport_name}")

        except TimeoutException:
            print(f"Timeout loading BBC Sport {sport_name}")
        except Exception as e:
            print(f"Error scraping BBC Sport {sport_name}: {e}")

        return items

    def scrape_country(self, country: str) -> Dict[str, Any]:
        """Scrape sports data for a specific country."""
        print(f"\n{'='*60}")
        print(f"Starting sports scraping for {country}")
        print(f"{'='*60}\n")

        # Get country configuration
        if country not in COUNTRY_SPORTS_CONFIG:
            raise SportsScraperError(f"Country '{country}' not supported. Supported countries: {list(COUNTRY_SPORTS_CONFIG.keys())}")

        config = COUNTRY_SPORTS_CONFIG[country]
        all_items = []
        sports_by_source = {}

        # Initialize driver
        self.init_driver()

        try:
            # 1. Scrape ESPN sports
            for sport in config['espn_sports']:
                self.random_delay(2, 4)
                espn_items = self.scrape_espn_sport(sport['name'], sport['url'], country)
                all_items.extend(espn_items)
                sports_by_source[f"ESPN {sport['name']}"] = len(espn_items)

            # 2. Scrape BBC Sport sports
            for sport in config['bbc_sports']:
                self.random_delay(2, 4)
                bbc_items = self.scrape_bbc_sport(sport['name'], sport['url'], country)
                all_items.extend(bbc_items)
                sports_by_source[f"BBC Sport {sport['name']}"] = len(bbc_items)

            # 3. Extract top sports and topics
            sports = [item.get('sport', 'Unknown') for item in all_items]
            sport_counts = Counter(sports)
            top_sports = [sport for sport, count in sport_counts.most_common(5)]

            # Extract topics from titles
            all_titles = ' '.join([item['title'] for item in all_items])
            top_topics = self.extract_topics(all_titles)

            # Generate summary
            summary = {
                'total_items': len(all_items),
                'overall_sentiment': 'neutral',  # Sports is generally neutral
                'top_sports': top_sports,
                'top_topics': top_topics,
                'sources_count': len(sports_by_source),
                'sports_breakdown': dict(sport_counts),
                'avg_virality_score': sum(item['virality_score'] for item in all_items) / len(all_items) if all_items else 0
            }

            # Build final result
            result = {
                'country': country,
                'category': 'sports',
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'last_updated': datetime.utcnow().isoformat() + 'Z',
                'items': all_items,
                'sports_by_source': sports_by_source,
                'summary': summary
            }

            print(f"\n{'='*60}")
            print(f"Scraping completed for {country}")
            print(f"Total items: {len(all_items)}")
            print(f"Top sports: {top_sports}")
            print(f"Sources: {list(sports_by_source.keys())}")
            print(f"{'='*60}\n")

            return result

        finally:
            self.close_driver()

    def extract_topics(self, text: str, top_n: int = 5) -> List[str]:
        """Extract top topics from text using simple word frequency."""
        # Common stop words to ignore
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                     'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
                     'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
                     'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that',
                     'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',
                     'what', 'which', 'who', 'when', 'where', 'why', 'how', 'all', 'each',
                     'every', 'both', 'few', 'more', 'most', 'other', 'some', 'such',
                     'says', 'after', 'over', 'new', 'vs', 'game', 'win', 'wins', 'beat'}

        # Clean and tokenize
        words = text.lower().split()

        # Filter words
        filtered_words = [
            word.strip('.,!?;:"\'-')
            for word in words
            if len(word) > 3
            and word.lower() not in stop_words
            and word.isalpha()
        ]

        # Count frequencies
        word_counts = Counter(filtered_words)

        # Return top N
        return [word for word, count in word_counts.most_common(top_n)]


def scrape_sports_data(country: str, headless: bool = True) -> Dict[str, Any]:
    """Main function to scrape sports data for a country."""
    scraper = SportsScraper(headless=headless)
    return scraper.scrape_country(country)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Scrape sports (100% free)')
    parser.add_argument('--country', type=str, required=True,
                       help=f'Country to scrape. Options: {list(COUNTRY_SPORTS_CONFIG.keys())}')
    parser.add_argument('--output', type=str, default='../output/',
                       help='Output directory path')
    parser.add_argument('--headless', type=bool, default=True,
                       help='Run browser in headless mode')
    args = parser.parse_args()

    try:
        # Scrape data
        data = scrape_sports_data(args.country, args.headless)

        # Save to output directory
        output_dir = os.path.join(args.output, args.country)
        os.makedirs(output_dir, exist_ok=True)

        output_file = os.path.join(output_dir, f"{datetime.now().strftime('%Y-%m-%d')}.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        print(f"\n‚úÖ Sports data saved to: {output_file}")
        print(f"üìä Total items: {data['summary']['total_items']}")
        print(f"‚öΩ Top sports: {', '.join(data['summary']['top_sports'])}")
        print(f"üì∞ Sources: {data['summary']['sources_count']}")

    except SportsScraperError as e:
        print(f"\n‚ùå Scraper Error: {e}")
        exit(1)
    except Exception as e:
        print(f"\n‚ùå Unexpected Error: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
